corpus = Corpus(VectorSource(energy$email))
# Let's see what's inside:
#corpus[[1]]
#corpus[[1]]$content
# We will use tm_map to process it:
#corpus = TermDocumentMatrix(corpus,control = list(removePunctuation = TRUE,stopwords = "english", removeNumbers = TRUE, tolower = TRUE))
corpus <- tm_map(corpus, content_transformer(stri_trans_tolower))
#inspect(corpus)
#corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords("english"))
corpus = tm_map(corpus, stemDocument)
# Just for interest, we can see which words are
# English stopwords:
#stopwords("english")
# Now, we are ready to apply bag of words:
dtm = DocumentTermMatrix(corpus)
# Look inside:
dtm
# To deal with this, we can consider removing words
# which appear extremely infrequently. The command below
# removes terms (= words) that appear in fewer than 3% of
# all documents (= do not appear in 97% or more of the
# documents).
dtm.sparse = removeSparseTerms(dtm, 0.97)
#Total number of words
dtm.sparse$ncol #number of words
dtm.sparse$nrow
email.df = as.data.frame(as.matrix(dtm.sparse) )
colnames(email.df) = make.names(colnames(email.df))
#names(email.df) = paste("w_", names(email.df),  sep ='')
# Next, we need to add the dependent variable.
# Recall that the dependent variable is numeric
# We want to do classification, so let's threshold it:
email.df$responsive = as.factor(energy$responsive);
table(email.df$responsive) #shows TRUE and FALSE numbers
#Top 5 Frequency terms
sort(colSums(email.df[,-789]), decreasing = TRUE)[1:5]
library(tm)
library(SnowballC)
library(stringi)
energy = read.csv("energy_bids.csv", stringsAsFactors=FALSE)
#str(energy)
corpus = Corpus(VectorSource(energy$email))
# Let's see what's inside:
#corpus[[1]]
#corpus[[1]]$content
# We will use tm_map to process it:
#corpus = TermDocumentMatrix(corpus,control = list(removePunctuation = TRUE,stopwords = "english", removeNumbers = TRUE, tolower = TRUE))
corpus <- tm_map(corpus, content_transformer(stri_trans_tolower))
#inspect(corpus)
#corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords("english"))
corpus = tm_map(corpus, stemDocument)
# Just for interest, we can see which words are
# English stopwords:
#stopwords("english")
# Now, we are ready to apply bag of words:
dtm = DocumentTermMatrix(corpus)
# Look inside:
#dtm
# To deal with this, we can consider removing words
# which appear extremely infrequently. The command below
# removes terms (= words) that appear in fewer than 3% of
# all documents (= do not appear in 97% or more of the
# documents).
dtm.sparse = removeSparseTerms(dtm, 0.97)
#Total number of words
dtm.sparse$ncol #number of words
dtm.sparse$nrow
email.df = as.data.frame(as.matrix(dtm.sparse) )
colnames(email.df) = make.names(colnames(email.df))
#names(email.df) = paste("w_", names(email.df),  sep ='')
# Next, we need to add the dependent variable.
# Recall that the dependent variable is numeric
# We want to do classification, so let's threshold it:
email.df$responsive = as.factor(energy$responsive);
table(email.df$responsive) #shows TRUE and FALSE numbers
#Top 5 Frequency terms
sort(colSums(email.df[,-789]), decreasing = TRUE)[1:5]
library(caTools)
library(rpart)
library(rpart.plot)
set.seed(144)
spl = sample.split(email.df$responsive, SplitRatio = 0.7)
email.train = email.df[spl,]
email.test = email.df[!spl,]
#build CART Classification tree
email.rpart = rpart(responsive ~ ., data = email.train)
prp(email.rpart)
# Predict on the test set:
email.rpart.predict = predict(email.rpart, newdata = email.test, type = "prob")
email.rpart.predict = email.rpart.predict[,2]
confMat = table(email.test$responsive, email.rpart.predict >= 0.5)
confMat
accuracy_rpart = sum(diag(confMat))/nrow(email.test)
accuracy_rpart
# AUC:
pred = prediction(email.rpart.predict, email.test$responsive)
library(caTools)
library(rpart)
library(rpart.plot)
set.seed(144)
email.df$responsive = factor(email.df$responsive)
spl = sample.split(email.df$responsive, SplitRatio = 0.7)
email.train = email.df[spl,]
email.test = email.df[!spl,]
#build CART Classification tree
email.rpart = rpart(responsive ~ ., data = email.train)
prp(email.rpart)
# Predict on the test set:
email.rpart.predict = predict(email.rpart, newdata = email.test, type = "prob")
email.rpart.predict = email.rpart.predict[,2]
confMat = table(email.test$responsive, email.rpart.predict >= 0.5)
confMat
accuracy_rpart = sum(diag(confMat))/nrow(email.test)
accuracy_rpart
# AUC:
pred = prediction(email.rpart.predict, email.test$responsive)
library(caTools)
library(rpart)
library(rpart.plot)
set.seed(144)
spl = sample.split(email.df$responsive, SplitRatio = 0.7)
email.train = email.df[spl,]
email.test = email.df[!spl,]
#build CART Classification tree
email.rpart = rpart(responsive ~ ., data = email.train)
prp(email.rpart)
# Predict on the test set:
email.rpart.predict = predict(email.rpart, newdata = email.test, type = "prob")
email.rpart.predict = email.rpart.predict[,2]
confMat = table(email.test$responsive, email.rpart.predict >= 0.5)
confMat
accuracy_rpart = sum(diag(confMat))/nrow(email.test)
accuracy_rpart
# AUC:
pred = prediction(email.rpart.predict, email.test$responsive)
pred = prediction(email.rpart.predict, email.test$responsive)
library(caTools)
library(rpart)
library(rpart.plot)
library(ROCR)
set.seed(144)
spl = sample.split(email.df$responsive, SplitRatio = 0.7)
email.train = email.df[spl,]
email.test = email.df[!spl,]
#build CART Classification tree
email.rpart = rpart(responsive ~ ., data = email.train)
prp(email.rpart)
# Predict on the test set:
email.rpart.predict = predict(email.rpart, newdata = email.test, type = "prob")
email.rpart.predict = email.rpart.predict[,2]
confMat = table(email.test$responsive, email.rpart.predict >= 0.5)
confMat
accuracy_rpart = sum(diag(confMat))/nrow(email.test)
accuracy_rpart
# AUC:
pred = prediction(email.rpart.predict, email.test$responsive)
auc_rpart = as.numeric(performance(pred,"auc")@y.values)
auc_rpart
pred = prediction(email.rpart.predict, email.test$responsive)
auc_rpart
library(randomForest)
set.seed(100)
email.rf = randomForest(responsive ~ ., data = email.train)
email.rf.predict = predict(email.rf, newdata = email.test, type = "prob")
email.rf.predict = email.rf.predict[,2]
confMat = table(email.test$responsive, email.rf.predict >= 0.5)
confMat
accuracy_rf = sum(diag(confMat))/nrow(email.test)
accuracy_rf
pred = prediction(email.rf.predict, email.test$responsive)
auc_rf = as.numeric(performance(pred,"auc")@y.values)
auc_rf
# Much better!
# Look at the variable importances:
varImpPlot(email.rf)
library(caTools)
set.seed(100)
# Now build the logistic regression model:
email.glm = glm(responsive ~ ., data = email.train, family = "binomial")
#summary(email.glm)
# Predict on the train set:
email.glm.predict.train = predict(email.glm, newdata=email.train, type="response")
confMat1 = table(email.train$responsive, email.glm.predict.train >= 0.5)
confMat1
accuracy_glm1 = sum(diag(confMat1))/nrow(email.train)
accuracy_glm1
# Predict on the test set:
email.glm.predict = predict(email.glm, newdata = email.test, type = "response")
confMat = table(email.test$responsive, email.glm.predict >= 0.5)
confMat
accuracy_glm = sum(diag(confMat))/nrow(email.test)
accuracy_glm
# Area under the curve plot
library(ROCR)
# The next three commands will give us the ROC curve
ROCpred = prediction(email.glm.predict, email.test$responsive)
ROCperf = performance(ROCpred, "tpr", "fpr")
plot(ROCperf, main = "Receiver Operator Characteristic Curve",
colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1),text.adj=c(-0.2,1.7))
# This last command will compute the AUC
AUC = as.numeric(performance(ROCpred, "auc")@y.values)
AUC
knitr::opts_chunk$set(echo = TRUE)
library(caTools)
letters = read.csv("letters_ABPR.csv")
set.seed(88)
letters$isB = as.factor(letters$letter == 'B')
#Split the data into 50-50
set.seed(1000)
spl = sample.split(letters$isB, SplitRatio = 0.5)
letters.train = letters[spl,]
letters.test = letters[!spl, ]
#Predict isB using CART
library(rpart)
library(rpart.plot)
letters.rpart = rpart(isB ~ . -letter, data = letters.train, method = "class")
summary(letters.rpart)
prp(letters.rpart)
prp(letters.rpart, extra= 6)
#baseline accuracy
#table(letters.train$isB)
383/(383+1175)
#out of sample predictions
letters.predict = predict(letters.rpart, newdata = letters.test, type="class")
# Build the confusion matrix:
confMat = table(letters.test$isB, letters.predict)
confMat
# Compute the accuracy:
accuracy_cart = sum(diag(confMat))/nrow(letters.test)
accuracy_cart
library(randomForest)
set.seed(1100)
letters.rf = randomForest(isB ~ . -letter, data = letters.train)
#variable importances
importance(letters.rf)
varImpPlot(letters.rf)
#predictions
letters.predict = predict(letters.rf, newdata = letters.test)
#out of sample accuracy
confMat = table(letters.test$isB, letters.predict)
confMat
accuracy_rf = sum(diag(confMat))/nrow(letters.test)
accuracy_rf
set.seed(2000)
#Split the data into 50-50 again with the letter variable
spl = sample.split(letters$letter, SplitRatio = 0.5)
letters.train = letters[spl,]
letters.test = letters[!spl, ]
#Predict isB using CART
letters.rpart = rpart(letter ~ . -isB, data = letters.train, method = "class")
summary(letters.rpart)
prp(letters.rpart)
prp(letters.rpart, extra= 6)
#baseline accuracy
table(letters.train$letter)
402/(394+383+402+379)
#out of sample predictions
letters.predict = predict(letters.rpart, newdata = letters.test, type="class")
# Build the confusion matrix:
confMat = table(letters.test$isB, letters.predict)
confMat
# Compute the accuracy:
accuracy_cart = sum(diag(confMat))/nrow(letters.test)
accuracy_cart
library(randomForest)
set.seed(3000)
letters.train$letter = factor(letters.train$letter) #for some reason, i get a numeric error and had to change this to a factor variable to make it read as a categorical data
letters.rf = randomForest(letter ~. -isB, data = letters.train)
#variable importances
importance(letters.rf)
varImpPlot(letters.rf)
#predictions
letters.predict = predict(letters.rf, newdata = letters.test)
#out of sample accuracy
confMat = table(letters.test$isB, letters.predict)
confMat
accuracy_rf = sum(diag(confMat))/nrow(letters.test)
accuracy_rf
boston = read.csv("boston.csv")
set.seed(148)
#Split the data into 70-30
spl = sample.split(boston$MEDV, SplitRatio = 0.7)
boston.train = boston[spl,]
boston.test = boston[!spl, ]
#Linear Regression without LAT and LON
linear_model = lm(MEDV ~ . -LAT-LON, data=boston.train)
#baseline accuracy
print('Baseline accuracy of training data is:')
mean(boston.train$MEDV)
#summary
summary(linear_model)
#out of sample predictions
boston.predict = predict(linear_model, newdata = boston.test)
# Compute OOS R^2:
SSE = sum( (boston.predict - boston.test$MEDV)^2 )
SST = sum( (mean(boston.train$MEDV) - boston.test$MEDV)^2 )
Rsq = 1 - SSE/ SST
Rsq
set.seed(148)
#Split the data into 50-50 again with the letter variable
spl = sample.split(boston$MEDV, SplitRatio = 0.7)
boston.train = boston[spl,]
boston.test = boston[!spl, ]
#Predict MEDV using CART
boston.rpart = rpart(MEDV ~ .-LAT-LON , data = boston.train)
summary(boston.rpart)
prp(boston.rpart, extra= 1)
#out of sample predictions
boston.predict = predict(boston.rpart, newdata = boston.test)
#Out of sample prediction accuracy
SSE = sum( (boston.predict - boston.test$MEDV)^2 )
SST = sum( (mean(boston.train$MEDV) - boston.test$MEDV)^2 )
Rsq = 1 - SSE/ SST
Rsq
library(e1071)
library(caret)
set.seed(148)
#setup 10 fold cross validation
tr.control = trainControl(method = "cv", number = 10)
#set the cp value at 0.001 at the start and increase up to 0.01 incrementally
cp.grid = expand.grid( .cp = (0:10)*0.001)
#run the train() to perform CV
tr = train(MEDV~ . -LAT-LON, data=boston.train, method="rpart", trControl = tr.control, tuneGrid = cp.grid)
plot(tr, metric = "Rsquared")
#plot the final tree model
prp(tr$finalModel, extra=1) # same as prp(boston.rpart.cv) when cp=0.004
tr$bestTune #will show the best cp value
#predict out of sample
# We can now run rpart() again, giving it this value of cp.
boston.rpart.cv = rpart(MEDV ~ . -LAT-LON, data = boston.train, cp = 0.004)
# The test set accuracy of this cross validated model:
boston.predict.cv = predict(boston.rpart.cv, newdata = boston.test)
#Out of sample prediction accuracy
SSE = sum( (boston.predict.cv - boston.test$MEDV)^2 )
SST = sum( (mean(boston.train$MEDV) - boston.test$MEDV)^2 )
Rsq = 1 - SSE/SST
Rsq
library(tm)
library(SnowballC)
library(stringi)
energy = read.csv("energy_bids.csv", stringsAsFactors=FALSE)
#str(energy)
corpus = Corpus(VectorSource(energy$email))
# Let's see what's inside:
#corpus[[1]]
#corpus[[1]]$content
# We will use tm_map to process it:
#corpus = TermDocumentMatrix(corpus,control = list(removePunctuation = TRUE,stopwords = "english", removeNumbers = TRUE, tolower = TRUE))
corpus <- tm_map(corpus, content_transformer(stri_trans_tolower))
#inspect(corpus)
#corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords("english"))
corpus = tm_map(corpus, stemDocument)
# Just for interest, we can see which words are
# English stopwords:
#stopwords("english")
# Now, we are ready to apply bag of words:
dtm = DocumentTermMatrix(corpus)
# Look inside:
#dtm
# To deal with this, we can consider removing words
# which appear extremely infrequently. The command below
# removes terms (= words) that appear in fewer than 3% of
# all documents (= do not appear in 97% or more of the
# documents).
dtm.sparse = removeSparseTerms(dtm, 0.97)
#Total number of words
dtm.sparse$ncol #number of words
dtm.sparse$nrow
email.df = as.data.frame(as.matrix(dtm.sparse) )
colnames(email.df) = make.names(colnames(email.df))
#names(email.df) = paste("w_", names(email.df),  sep ='')
# Next, we need to add the dependent variable.
# Recall that the dependent variable is numeric
# We want to do classification, so let's threshold it:
email.df$responsive = as.factor(energy$responsive);
table(email.df$responsive) #shows TRUE and FALSE numbers
#Top 5 Frequency terms
sort(colSums(email.df[,-789]), decreasing = TRUE)[1:5]
library(caTools)
library(rpart)
library(rpart.plot)
library(RO)
library(caTools)
set.seed(100)
# Now build the logistic regression model:
email.glm = glm(responsive ~ ., data = email.train, family = "binomial")
#summary(email.glm)
# Predict on the train set:
email.glm.predict.train = predict(email.glm, newdata=email.train, type="response")
confMat1 = table(email.train$responsive, email.glm.predict.train >= 0.5)
confMat1
accuracy_glm1 = sum(diag(confMat1))/nrow(email.train)
accuracy_glm1
# Predict on the test set:
email.glm.predict = predict(email.glm, newdata = email.test, type = "response")
confMat = table(email.test$responsive, email.glm.predict >= 0.5)
confMat
accuracy_glm = sum(diag(confMat))/nrow(email.test)
accuracy_glm
# Area under the curve plot
library(ROCR)
# The next three commands will give us the ROC curve
ROCpred = prediction(email.glm.predict, email.test$responsive)
ROCperf = performance(ROCpred, "tpr", "fpr")
plot(ROCperf, main = "Receiver Operator Characteristic Curve",
colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1),text.adj=c(-0.2,1.7))
# This last command will compute the AUC
AUC = as.numeric(performance(ROCpred, "auc")@y.values)
AUC
accuracy_glm1
accuracy_glm
AUC
library(glmnet)
library(glmnetUtils)
# When doing cross validation, we must always set our seed,
# because the folds are randomly selected.
# Let's do ten-fold CV.
set.seed(55)
email.glmnet.cv = cv.glmnet(responsive ~ ., data = email.train, family = "binomial", nfolds = 5)
# Plot the results of the CV
plot(email.glmnet.cv)
# Find the optimal value of lambda:
lambda.min = email.glmnet.cv$lambda.min
lambda.min
# Which words have non-zero coefficients in the optimal
# cross validated model?
coeffs = coefficients(email.glmnet.cv, s = "lambda.min")
row.names(coeffs)[ which(coeffs != 0) ]
# Find the out of sample accuracy:
email.glmnet.predict = predict(email.glmnet.cv, newdata = email.test, type = "response", s = "lambda.min")
confMat = table(email.test$responsive, email.glmnet.predict >= 0.5)
confMat
accuracy_glmnet = sum(diag(confMat))/nrow(email.test)
accuracy_glmnet
# Find the AUC:
pred = prediction(email.glmnet.predict, email.test$responsive)
auc_glmnet = as.numeric(performance(pred,"auc")@y.values)
auc_glmnet
reinstall_tinytex()
install.packages('tinytex')
library(tinytex)
install.packages('xcolor.sty')
library(xcolor.sty)
tinytex::reinstall_tinytex()
tinytex::tlmgr_update()
options(tinytex.verbose = TRUE)
knitr::opts_chunk$set(echo = TRUE)
# Run K-Means Clustering
# Set the seed, and run k-means with 4 clusters:
set.seed(88)
airline.kmeans = kmeans(airlinesNorm[,], 5, iter.max = 1000)
library(caret)
#load data
airlines = read.csv("AirlinesCluster.csv")
summary(airlines)
#Normalize data
airlines.preproc = preProcess(airlines[,])
airlinesNorm = predict(airlines.preproc, newdata = airlines)
#Check normalization
summary(airlinesNorm)
sd(airlinesNorm$Balance)
library(caret)
#Calculate distance
distances = dist(airlinesNorm[,], method = "euclidean")
#Hierarchial Clustering
# Next, we use hclust() to do the hierarchical clustering.
airline.clust = hclust(distances, method = "ward.D")
# We can plot the dendrogram with plot() on the hclust() object.
plot(airline.clust)
# Let's cut the dendrogram to get 5 clusters:
clusterGroups = cutree(airline.clust, k = 5)
# To check the spread of the observation data among clusters
summary(clusterGroups == 1)
# Use aggregate() to compare average values of each variables in each clusters using both normalized and unnormalized data
# aggregate() generalizes tapply() to multiple variables:
aggregate(airlinesNorm[,], by = list(clusterGroups), mean)
aggregate(airlines[,], by = list(clusterGroups), mean)
# Run K-Means Clustering
# Set the seed, and run k-means with 4 clusters:
set.seed(88)
airline.kmeans = kmeans(airlinesNorm[,], 5, iter.max = 1000)
# Check for # of observatiosn in clusters
summary(airline.kmeans$cluster==5)
summary(airline.kmeans$cluster==4)
# table(airline.kmeans$cluster)
# Check for centroids
airline.kmeans$centers
# Compare centroids with the hierarchical clustering & Kmeans clustering
aggregate(airlinesNorm[,], by = list(airline.kmeans$cluster), mean)
# Run K-Means Clustering
# Set the seed, and run k-means with 4 clusters:
set.seed(88)
airline.kmeans = kmeans(airlinesNorm[,], 5, iter.max = 1000)
# Check for # of observatiosn in clusters
summary(airline.kmeans$cluster==5)
summary(airline.kmeans$cluster==4)
# table(airline.kmeans$cluster)
# Check for centroids
airline.kmeans$centers
# Compare centroids with the hierarchical clustering & Kmeans clustering
#aggregate(airlinesNorm[,], by = list(airline.kmeans$cluster), mean)
aggregate(airlinesNorm[,], by = list(clusterGroups), mean)
aggregate(airlinesNorm[,], by = list(airline.kmeans$cluster), mean)
version
setwd("C:/Users/sikyu/Desktop/UCLA/Data Science Project/UCLA Datathon/COVID-19/COVID-19")
version
